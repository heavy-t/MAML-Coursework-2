{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64b51d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carefully modify the below two string variables. Ensure there are no typos.\n",
    "\n",
    "student_id = \"10857988\" # set this to your student ID\n",
    "\n",
    "student_mail = \"thomas.roberts-9@student.manchester.ac.uk\" # your email address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef390d5",
   "metadata": {},
   "source": [
    "# Coursework 2\n",
    "\n",
    "This coursework test contains several Jupyter Notebook cells with the comment `# TODO`. This is where you type the code for your solutions. Do not alter any of the other cells. \n",
    "\n",
    "It is good practice to include markdown cells explaining your work, but in this test they won't be marked. \n",
    "\n",
    "Here are some tips:\n",
    "\n",
    "* **Do not alter the names of the predefined variables and functions,** such as `h_best_L1`, `astro_scores`, etc. The (return) values of these variables and functions will inform the marking. Renaming them and failure to follow the problem description will result in loss of marks.\n",
    "\n",
    "* **Ensure that functions *return* values, not merely print them.** Each function should have at least one occurance of the `return` keyword, followed by a variable of the type required by the question.  \n",
    "\n",
    "* **Do not hard-code any solution variables.** All problems must be solved by computer code using the data in the provided CSV file. For example, do *not* simply define a variable `astro_scores = 1234` with a fixed value. Your Jupyter Notebook should produce results with a modified data file that has the same format but different numerical (or NaN) values.\n",
    "\n",
    "* **Avoid inefficient computations.** Ensure that each cell can be run in about 20 seconds on a modern laptop. Long-running cells will be timed out which will result in loss of marks.\n",
    "\n",
    "* **Submit this test as a single .ipynb file using Blackboard.** You can simply keep the name `test2-2025.ipynb`. There is a basic testing code at the end that verifies some parts of the coursework.\n",
    "\n",
    "   <span style=\"color:blue; font-weight:bold\">Strict deadline: Monday, 24th of March 2025, at 1pm. There are no automatic extensions.</span>\n",
    "\n",
    "### Note on independent work\n",
    "\n",
    "You need to complete all coursework tests independently on your own, but you are allowed to use online resources and all course notes and exercise solutions. The course notes from chapters 1 to 5 contain all that is required to solve the below problems. You are not allowed to ask other humans for help. In particular, you are not allowed to send, give, or receive code or markdown content to/from classmates and others.\n",
    "\n",
    "The University Guidelines for Academic Malpractice apply: http://documents.manchester.ac.uk/display.aspx?DocID=2870\n",
    "\n",
    "**Important: Even if you are the originator of the work** (and not the one who copied), the University Guidelines require that you will be equally responsible for this case of academic malpractice and may lose all coursework marks (or even be assigned 0 marks for the course)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295b152",
   "metadata": {},
   "source": [
    "# Start of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fdc2a253-d922-4124-931c-d4a90308d222",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34389c5497ccc8a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a10eb-93d7-44fa-bbf3-9c3de7025d2e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-404456cc35ae4faa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 1a\n",
    "\n",
    "Consider a supervised learning problem on a $d$-dimensional feature space $\\mathcal{X}=\\mathbb{Z}^d$ with integer coordinates, and a $J$-dimensional label space $\\mathcal{Y} = \\mathbb{R}^J$ (notation as in the semester 1 lecture notes). Assume that the loss function is the $L^1$ loss, i.e., $L(y,y') = \\| y - y' \\|_1$ with the 1-norm. Further assume that we are given data pairs $(x_1,y_1),(x_2,y_2),\\ldots,(x_N,y_N) \\in \\mathcal{X}\\times \\mathcal{Y}$.\n",
    "\n",
    "Now consider the best $L^1$ hypothesis given the data, i.e., the optimal hypothesis $h$ that minimizes the empirical error \n",
    "$$\n",
    "\\hat{R}(h) := \\frac{1}{N}\\sum_{n=1}^N L(y_n, h(x_n)).\n",
    "$$ \n",
    "Implement a function `h_best_L1(x, X, Y)` that evaluates this best hypothesis for a given feature point `x`.  \n",
    "\n",
    "The inputs of `h_best_L1(x, X, Y)` are\n",
    "\n",
    "* a $d$-dimensional NumPy vector `x` which can always be assumed (without checking) to be among the feature vectors $\\{x_1,\\ldots,x_N\\}$\n",
    "\n",
    "* an $N\\times d$ NumPy matrix `X` of the features\n",
    "\n",
    "* an $N\\times J$ NumPy matrix `Y` of the labels\n",
    "\n",
    "The function returns a $J$-dimensional NumPy vector. \n",
    "\n",
    "All data types are standard floats, even if we only ever use integer values in `x` and `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6b3dc6c4-7547-4614-b0a7-7bab6a2d05e6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ducks-mean",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "h_best_L1 = None\n",
    "\n",
    "# TODO: Provide your solution code here that defines the function `h_best_L1`\n",
    "\n",
    "\n",
    "def h_best_L1(x, X, Y):\n",
    "    import numpy as np\n",
    "\n",
    "    # Retrieve the indices where X matches x\n",
    "    indices = np.where((X == x).all(axis=1))[0]\n",
    "    \n",
    "    # Pick the matching Y values with the retrieved indices\n",
    "    Y_pick = Y[indices]\n",
    "    \n",
    "    # Return the median across the picked Y\n",
    "    return np.median(Y_pick, axis=0)\n",
    "\n",
    "### Good  to go!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "253f1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "[3. 5.]\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\twrob\\Desktop\\Year 3\\MAML\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\twrob\\Desktop\\Year 3\\MAML\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:137: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Testing\n",
    "def test_h_best_L1():\n",
    "    # Test 1: Basic functionality\n",
    "    X = np.array([[1, 2], [3, 4], [1, 2], [5, 6]])\n",
    "    Y = np.array([[10, 20], [30, 40], [15, 25], [50, 60]])\n",
    "    x = np.array([1, 2])\n",
    "    expected = np.array([12.5, 22.5])\n",
    "    assert np.allclose(h_best_L1(x, X, Y), expected), \"Test 1 Failed\"\n",
    "    \n",
    "    # Test 2: Single match\n",
    "    x = np.array([3, 4])\n",
    "    expected = np.array([30, 40])\n",
    "    assert np.allclose(h_best_L1(x, X, Y), expected), \"Test 2 Failed\"\n",
    "    \n",
    "    # Test 3: No match (should return NaN)\n",
    "    x = np.array([7, 8])\n",
    "    expected = np.array([np.nan, np.nan])\n",
    "    assert np.all(np.isnan(h_best_L1(x, X, Y))), \"Test 3 Failed\"\n",
    "    \n",
    "    # Test 4: Multiple dimensions\n",
    "    X = np.array([[1, 2, 3], [4, 5, 6], [1, 2, 3], [7, 8, 9]])\n",
    "    Y = np.array([[10, 20, 30], [40, 50, 60], [15, 25, 35], [70, 80, 90]])\n",
    "    x = np.array([1, 2, 3])\n",
    "    expected = np.array([12.5, 22.5, 32.5])\n",
    "    assert np.allclose(h_best_L1(x, X, Y), expected), \"Test 4 Failed\"\n",
    "    \n",
    "    # Test 5: Large dataset\n",
    "    X = np.random.randint(0, 10, size=(1000, 5))\n",
    "    Y = np.random.randint(0, 100, size=(1000, 3))\n",
    "    x = X[500]  # Choose an existing row\n",
    "    expected = np.median(Y[np.where((X == x).all(axis=1))[0]], axis=0)\n",
    "    assert np.allclose(h_best_L1(x, X, Y), expected), \"Test 5 Failed\"\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "    X = np.array([[1, 2], [2, 3], [1, 2], [1, 2]])  # Example feature points\n",
    "    Y = np.array([[3, 5], [6, 8], [3, 5], [4, 7]])  # Corresponding labels\n",
    "\n",
    "    x_test = np.array([1, 2])  # Test feature point\n",
    "    result = h_best_L1(x_test, X, Y)\n",
    "    print(result)\n",
    "    print(isinstance(result[0], float))\n",
    "\n",
    "# Run tests\n",
    "test_h_best_L1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d064c0",
   "metadata": {},
   "source": [
    "## Problem 1b\n",
    "\n",
    "Write a function `best_L1_err(X, Y)` that, given data as in Problem 1a, returns the empirical error of the best $L^1$ hypothesis as a floating point number.\n",
    "\n",
    "(The function `best_L1_err` may of course make use of `h_best_L1` by calling it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d999032",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_L1_err = None\n",
    "\n",
    "# TODO: Provide your solution code here that defines the function `best_L1_err`\n",
    "\n",
    "def best_L1_err(X, Y):\n",
    "    \n",
    "    # Get the total number of feature points in X\n",
    "    num_points = X.shape[0]\n",
    "    \n",
    "    #Set the empirical  error to 0\n",
    "    error = 0.0\n",
    "\n",
    "    # Iterate over each feature point to calculate the error for each X[i]\n",
    "    for i in range(num_points):\n",
    "        h_best_L1_value = h_best_L1(X[i], X, Y)\n",
    "\n",
    "        #Add the error for X[i] each iteration\n",
    "        error += np.linalg.norm(Y[i] - h_best_L1_value, ord=1)\n",
    "\n",
    "    empirical_error = error / num_points\n",
    "\n",
    "    # Return the corrected empirical error\n",
    "    return empirical_error \n",
    "\n",
    "### Good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ae58bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "#Testing \n",
    "import numpy as np\n",
    "\n",
    "def test_best_L1_err():\n",
    "    # Test 1: Basic functionality\n",
    "    X = np.array([[1, 2], [3, 4], [1, 2], [5, 6]])\n",
    "    Y = np.array([[10, 20], [30, 40], [15, 25], [50, 60]])\n",
    "    expected = np.mean([np.linalg.norm(Y[i] - np.median(Y[np.where((X == X[i]).all(axis=1))[0]], axis=0), ord=1) for i in range(len(X))])\n",
    "    assert np.allclose(best_L1_err(X, Y), expected), \"Test 1 Failed\"\n",
    "    \n",
    "    # Test 2: Single point dataset\n",
    "    X = np.array([[2, 3]])\n",
    "    Y = np.array([[5, 10]])\n",
    "    expected = 0.0  # Single point should have zero error\n",
    "    assert np.allclose(best_L1_err(X, Y), expected), \"Test 2 Failed\"\n",
    "    \n",
    "    # Test 3: No variance in Y\n",
    "    X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "    Y = np.array([[10, 10], [10, 10], [10, 10]])\n",
    "    expected = 0.0  # All Y values are the same, so median matches, and error is zero\n",
    "    assert np.allclose(best_L1_err(X, Y), expected), \"Test 3 Failed\"\n",
    "    \n",
    "    # Test 4: Multiple matches per X value\n",
    "    X = np.array([[1, 1], [2, 2], [1, 1], [2, 2]])\n",
    "    Y = np.array([[5, 15], [10, 20], [7, 17], [12, 22]])\n",
    "    expected = np.mean([np.linalg.norm(Y[i] - np.median(Y[np.where((X == X[i]).all(axis=1))[0]], axis=0), ord=1) for i in range(len(X))])\n",
    "    assert np.allclose(best_L1_err(X, Y), expected), \"Test 4 Failed\"\n",
    "    \n",
    "    # Test 5: Large dataset\n",
    "    X = np.random.randint(0, 10, size=(1000, 5))\n",
    "    Y = np.random.randint(0, 100, size=(1000, 3))\n",
    "    result = best_L1_err(X, Y)  # Just ensuring no errors and returns a valid float\n",
    "    assert isinstance(result, float), \"Test 5 Failed\"\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "    # Test the function\n",
    "    X = np.array([[1, 2], [2, 3], [1, 2], [1, 2]])  # Example feature points\n",
    "    Y = np.array([[3, 5], [6, 8], [3, 5], [4, 7]])  # Corresponding labels\n",
    "\n",
    "    result = best_L1_err(X, Y)\n",
    "    print(result)\n",
    "\n",
    "# Run tests\n",
    "test_best_L1_err()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAABKCAYAAADE1fMXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABVgSURBVHhe7Z0HkBRFF4AbMGcxYc4iZsWICGIuBTGVKJS5EBOWmTP/WgoFnqIiQQUVVFSMmLDEhIoYUEyIEVTEiCjKGRDdf7/n9Dk3zOzM7s3MzbDvq5q63dm92Qndr1/q180KRYyipMjHH39snnrqKTNt2jTTu3dvs8UWWzifVMbff/9t5s6da1q2bOnsUZSFae78VZTUWG+99Uzbtm3Nr7/+alZeeWVnb3kg4ObMmWNeeeUVc+GFF5r77rvP+URR/FFhp6TOUkstZX788Uez7rrrmhVXXNHZWx4//fSTmTBhgllsscVMixYtnL2KEowKOyV1/vrrL/Phhx+aNm3amGWWWcbZWx6rrrqqOfTQQ80222wjwlNRwlBhp6TCn3/+ad5++23zzDPPmK+++kr8dptssonzqaIkjwo7JXG+/PJLc+mll8rr1q1bmxtvvNHU1dWJGasoaaHCTkkUfGvXXnut2W+//cx2220nAq5Dhw5mgw02MKussorzLUVJHk09URLl4YcfNo8++qgZMGCAWW211cRfN3DgQInC9uzZU74ze/ZsM27cODN//nx5H8TSSy9tDjrooAZBjT/++MP079/frL766ubUU0919irKwqiwUxLDCiKE1Nlnn20WX3xx0fTOP/98c+KJJ5p27do536wcFXZKVNSMVRIDTe3bb781G264oQg6+OabbyRYgTn7zjvvmF9++UX2K0rSqLBTEgONDkG35JJLynsSgV966SWzzjrryGcffPCBWWKJJeQzRUkaNWOVRPn888/NLbfcYjp16mQ++ugjmRr20EMPmW233dbstNNOErSoBDRCAh8c/4svvhDhufbaa8txe/XqpUJUWQgVdkrioNH9/vvvkkDcvHlzCVKwVZpQrCiVoMJOUZSqQH12iqJUBSrsFEWpClTYKYpSFaiwUxSlKlBhpyhKVaDRWCURSBi+/vrrnXfpsOmmm5ozzjhDcu6UbELKESX5d9ttN6lJ6Mdvv/1mnn32WbPPPvvE+ixV2CmJMGvWLFNTU2NmzJjh7DEyiZ8G3KxZM2dPeXz33Xfm66+/llp4JChTlt0NeXskGm+99dbOHiVLkG959913S8FVbzK5N/eSdvP666+bI444IrZK1CrslMR48803zSWXXCJrTQDzYfv27StTyBoLzZbS7pMmTTIPPPCA+eyzz2T/UUcdJQUBtFR79qA9vPfee+aYY45p8HwQctQ45HnSXhB4PF8q5vD6gAMOcL7ZOFr8r4jzWlFiZY011pBiAFQoBqZ4ofFhwtj5spWCdkhHoBho586dzVprrSWmM4UHqKaywgorON9UsgAzaO68807R7r11DJlVw7PceeedTatWrWQfz5dnyNRCtMA4zFkNUCiJwejdrVs3s8ceezh7jJgm999/v5g0cUFFFTrRTTfdZJZffnn5DSVbYJZSjmvNNdd09vwHgg2Bhs/VDWW7qF34ySefOHsaR1UKO1TkN954Q/w7PIA8waT3fv36SV24PLDccsuZk08+uUEJdpY9nDx5svMuPliikWUVo5aOuuOOO8y7777rvFPCmDlzphk0aFCDIqv0penTp8uSlvSlefPmSWUbni/mqWXq1KnyfKxPzsL3WZeEY3g9agxiaO5xPaNEhd3PP/9sXnvtNXPvvfeaESNGZKZhvfXWW3JOxx9/fO5Wplp//fWlxPlVV10lPo48gI/uuOOOk2UPgWgbkVoEd9xQ7h0HOP6hMDCt/vnnH+edEgbaOILOfc8QUqwUh+bWp08f6VcspETbrK2tFQHI/+G+wNXghgGb76PBXXfddb7tgUo2BKXiUEoSFXac5K233mpGjhxpbr/9dhkZ3CD5hw4davbcc0+xzb2SPQm46TfffLNUys3rGgg77LCD2X777eU68qKZ7rvvvuboo4923v2rJeDDQeDECSbRYYcdJiWllOQh4EBbZACj1H6PHj3EVEUjQwiy2BL9fO7cuSK43PC/u+++uwyC/L+fHxdXCH2Wgq+NJVFhR+2y4cOHB5bL5gKnTJliFixYIBde6QWxNB+aY9gaBowwRO623HJLs/nmmzt78wcdGu2OUZVoZB6g0ZJG4E45IN/qiSeeSGWQU+KH50Y7xE9KW2QQtoEEG4wqNRgTkEAo4lKiPwbl3cVFKj67oDQAoi2kClDEkQWPKzUpGTnIuQozSUhPwLew1157BZ5TXmDxGhz/jz32WG5Km6NJM/DROSwMUrgVlPxhI+KkFpED6V4HmDxIIqu00yDo77RdBuz27dvXl+5PiiYNUHCzEDysNoWfpVKoVouWWApGoeeff95stNFGZuONN3b25hfuHYPEtGnTZMsLaPvMcrD+OzoKLo68+B+VhcFfR/qIjbSizZEGtOuuu4o/zuIXVEMooqRsttlmkqLkTRQHBsk4BGGTCrs4IJrz9NNPO++CwWeAydymTZuFIkJ5BYcvAQsaSV5MQTvAMZPCwvnfdtttDaJ3Sn749NNPzbLLLltvLRF5RWPDR8s+NDiEnk0ud4NW37ZtW/kOQtOt9QMaI77AOIRdrEnFdDhODuGDufj999+LGouT8uWXXxazCwkONGxuCloZn/EdImn2hiHtMTvHjBkjpg43FAfmiy++KCFsLp7fopPweyuttJKMAOxjHdKWLVs2MFXxKdxzzz3ivPZbiZ7jEgJH+yMl4YUXXhDfHqY2WiPZ3K+++qp58MEHJXWC88YxW645zHXiN0TwujeENoILiFo//vjj9Z9x7e4Vuiz8NiMj54IZkJd1F7gOfDRcm9XoeNaYPeRaIRDTgPQI2oJNZLWgmTz33HNm4sSJ0n69HRDfMgEW2gYaTZZhNTf8ogQLCRBYjRr/NW2e/SyAFOWek12BFYGvzR1Zp09wHzgW7Zt+gvbu1upo3wQssEbc94znz/8wBZA27A0a0h+xxOLwscf2pGgALKzCwsdcGI5LTpyQMn4lL0ThuElXX321JIPipLSJpvwlUnfXXXeZQw45xAwePNh0795dJgezIVT5DSK5BDa48SQeIvg4B47rjfLROBGWQU5QPkcIIdRQwQmDEzgB/tIxOacJEybI51F8hH5w3qTjjB07Vsw3Nhz17kg1DYDOxu/RIBkU7Lm4QWjQUWlkecm7s9ARTj/99HpBwvVxLxjdmxIsgGuuuUYGY9o06RQ42i20TTIMWPvW/cyyCPeSPsEAQhsjR85qzwh69JwhQ4bIrJNK4X6hYJDGdeyxx5r999/fXHTRRQsNICg5fu2U75922mmSHYGy44a+wj1G6YiDWIQdDQDBhJBhbhsXzYiJFGcyOCOHF0bFK664wtxwww0NRgBAIxw/frzp0qWL3DRGAr7Dca3D0/4/GwIM/wBCE2HHPj53w+iBsAvSfgiZjx492ncE4Vgk8pKE3FjwWXGOF198cf3oSENA47R06NBBplSxUhYN1O96LAwoNCA/E8EPGvYFF1wgib6VbMOGDYvN3CQye+SRRzrv/j032kNTCW4GUTRqNAxmZNBeEBg//PCD8w1j6urqZPChfScdPWwMmJG0Z4QQEU+uDYXA+rbR8mh3aLGVZkEA94dj0y8xRYNcRPRjzgMNzg0aJYnnfmYqx8YP6GeJVUIswg4zCu0N29tbcYJQNOZeEFys1xTg5vMQ0PbcHQstAGFUrukIaA6Yttj/QXAe3nNxU8nvBoHvEGEGXCemqoVOjwmFsA8SchauhwYclnZjYb7qZZddJtpLJRudx69hVgL3k2i8239HMjA5l1bLTxO0FDqYzRvDZEPb4J5ZeDakOuFy8JuvibaPRlKJ1h8nuG0QQPh1GegR0ERLrTBCATnppJOkfeFvc0Ofs0IxCAQc9+eRRx6RPsxrBoIgeNZYaUzli5I9gGWGdXPwwQfH1u9iEXY4GfGT2cWPGwtCCb8c2iIXi1pMB0B7oWICI0HeoZFh6gMNC/OYDk4jwk9BA/SWwfGj3GCLHUmZc1jJVml6UBC0F2ZXuEdvzHdM/bThXGhfXCcDOILXG1FEGKJ5Mlj5dUL8zxyD74XBoM7UNkzKcrew6DX38/DDD5fXDKYMhh07dmwwUCHEMRHdPkkEHRVImKETJvD4DSwUsimiTNbHCunatasZN25cycGMz/Cfo2F7E5EbQyzCDkEHaEVRHJ1hIAjOOuss0XwQcPit8P2hBRAcKHfUR4A09Ujrxy677FJvNvNw6WA0YoIjBHO8ztpFFYIvmMjWrKfT+GXTJw2/aYNEVkDgirFCjXaHBoM2HVSmCtMQf57X/5Q2tB02+g/KCFqdN+UKH5o1Py08g7333tuccMIJJQdS76BJny1lFVm4b+TUloJjM+MGSzFOYhF2SYBmh7+KChn4rLh4IDJLgKAURKDYLNy8KA8ibWiMtiIIAwZaAWkYmANRi1yGjb5eEPyYWZhslWylMuIbA35K/Hd0NiJ5ZOM3Fdx/ngMdk7xMC/uJfqNteP3MFnxT7dq1i2R6IVwZ0Hfccceyt6gDISY3AnqrrbZq4MJBcKNVciw3tDm/CiRxgt+81P2hr8ZtQUAsEsA2CNTycrUuP3hAo0aNkmPhoGSkufzyy0VdZrQhtF0KRjJvVj6diAhqlqKWNCxMCxu5ooorJgTzBb3RrCC4Hu5J1LQTImdXXnmlaB+VbKTlxBWgcINfCT8lJi3FGqMI+qRg4CEKiCaE1mKx/jraO24GnO22uAVtFbMX8zuJ+1MpmNMMiF6zm3aD5urWUBkEgyqQLArEIuwYNbhp5NF4o4JWkygHgglMISEq6wbTgBGVnLowrElkQVNEGJdy5NPBgkYcroORsBQIWNR/K6ijgN+DQghAo0RzQgBG0QwAs5d7Uirw4gYhOmDAAIkIV7KdcsopsQUoLFwDkW6SjYm4R732pEF7cg8iCGSEBK4H2hfuBtwjPGt8ysC+KEnuaeMOstCWcQ1hols/G9cVVoEkSxDAoM2QvoT7JwqxCDsiPvjTEHaMDLajc1OJvqANgG0sjHw0EqIyJCoihNjHe2smEe5/8sknG4ySdAoukvQNC+F/BAa/zef8Ng/KPU8P+A6/Y/2LfjBaW/OJc2EUx3TjnEnMJLXGClHy+4hEWXMZIcr5Et6n4Ud9AHRshJv1j1CtI2oCJfcGbYNrc2sgeYLnSb4kQphcyiwIOnxRDKgMtrY9kvSMCwVoc7Q1NrQ8ngHPD9PPvs4KtoYc5rfV1hiUif67XQVRKpBkCfoZGSAoIKQLRVEuYplBgUbEg0YIkcRIZBHBxowFtABGENR7bij7eM8DwCyi/BMNCuFA8MH6Q3g42O18jtDhmPwvyYdkcFszh5EXYUvBQKI8JE+S/sKDc/vp+D5OZyJPNuXDC99HSKJZ0rgRXuQqcTPRKjGxEN5oqmy85vs4pemk3HBuPh3YbZ6GQfQZ4Y7p1KtXL9+8RD/QovFpkiqB76UpTb9KQFijBXOvaQthaTZxQ1STgcL7nGibtMMxY8ZIp8KXirnau3dvEWY8d0xWIotYNGhHmLxcB5od1Zm9sy6aCgQz/YiEbQZmGwijIIM7qwHNj40+xPXgOvIOPH4zKJoKrokAC9eEvCAvNfScitI+VoqColDUhgrFG1Moajuyr9j55X1dXZ28D4NjFLUwec0x+F+OWRQmss8P+7tBv8HngwYNKtTU1EQ+D75XFCiFohYq7+1v2OsKYuzYsYUpU6Y475Jj6tSphS5duhSKHdHZkx+4l8OHDy/07NmzUNTEnb3pMmTIkJLPybY9dxugDfq1Mz4fNmxYYeDAgXJtWcP2Qfe1eClaMIXiYFuYOHGis6chM2bMKNTW1sqxssKcOXMKgwcPjnROsYcoGQ0YoTGrrG8HKcz7qOo9x7C+Eo4RJbRtfzfoN/gcbYvkSkbgKHAsRj+rMdnfKOWzQlthtHH7SJKg+OxEU0WL9ZrsWYdzRwMnaZQy6phaWcS2PXcboA36tTNMWrQ9NAzMXzSgLGH7oPtavESpQJI16Gv4q7m+MGIXdlkGXxipHnSyKDZ+JRD9QuAlPZUIsxfTnWlNYcmcWQOfEWbVOeec0yAamGdwtSAIERS4axCIeYPnUqoCSdagD+PeiloerqqEHSM1uVyMWgQ04qZo3sgE/wMPPLCk9tdY0IyI+KHVkZicJ8iRJNpH1DqJXDruDQUd8E2lCdoFmhP+XYSed72FPMDAQxsmBQo/cJJtuLHwnKmsRCAlakCPf6o6Jk+eXChqFYXZs2c7e+KhaL4UiqZAoE8kLpI6/6SZPn16oXv37oXRo0cn5tfiN7g3PIswwnx25YLfKKo/OIvQbvHpBfmks+Szo/3w7ObNm+fsCaeqNDsLGgWpMqTExDkjgKk3mDFJRkVJq0Gro7pM1Cz6LIBPy+bSoV3j/4wb/DfkEKKhEOEOA/O/lB+4XDD/ovqlswjtFp9ekEZnfelx3rNK4VyY6eEtYlCKZkg857WiJEJRE6gvj3XuuefG7mPEqU5qE79BHiXLNEYpoqBUF1Wp2SnpgaCjRh35aeR2xSnoSPymQgyzOsjT4zfIoXTPZ1UUi2p2SmIQLcNVwIwSKlhHTZb2gzm9CLP58+dLWgeJsX6pEVRPYcqZonhRYackAs2KGSgU/FzgU1I+CUj3wZRdFFaPU+JHhZ2SCCRuU6UGTSwtmLJIeai85R0q6aDCTlGUqkADFIqiVAUq7BRFqQpU2CmKUhWosFMUpSpQYacoSlWgwk5JFcpflbsimh9MEaNuHIvexDm/WVl0UWGnpAaCLuoCzKWgDFFtba2UQGcubL9+/aRwpqKUQvPslNSgqbFGB5UqKl2XlGNQb42paEwLo1LHrFmzRPidd955sa4gryxaqGanpAaCqbELMKPVsQAOx7CltCiaCe+//778VRQ/VNgpqRDXAswsa4mvzl03jlWlWPqQYytKECrslMQJWoAZU5QyTWhrYRvfIyjBa4SdHxQITXMurpIvVNgpiRO0ADNRVAoGsDZr2GYXMioFwhOBqCh+qLBTEodFlVu3bi1LP7I4il15jUCFXeA7bKMoZ9ZXqVeyjQo7JXFYmwEzdNKkSaZ9+/aNWrWKYATmsB/sj7J+qFKdaOqJkgqsETFixAjTt29fWWOVhbEp6kmxTXx6YbRq1cqceeaZYgrX1NSYbt26mU6dOslnCNI+ffqYzp07yzq6iuKHCjslFYYOHSqrUvXo0cOMHz9eBFMlGh7NddSoUfLanWfXv39/EXiaZ6cEoWaskgpxLcCMcENQssg5qSysRTFy5EjTtWvXXC5MraSHanZKKtDM6urqJMgQx0rzRF5nzpwpx2Q1MS3FroShwk5RlKpAzVhFUaoAY/4POVWURM37CH4AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "ab13b0c0",
   "metadata": {},
   "source": [
    "## Problem 2a\n",
    "\n",
    "Using only plain Python with no modules except NumPy, write a function `my_knn(x, X, k)` that takes as inputs a $d$-dimensional NumPy vector `x` and an $N\\times d$ NumPy array `X` (each row corresponding to a data point). The parameter `k` is a positive integer. The function returns a Python list with the indices of $k$ nearest neighbours to `x` in `X`, where distance between two $d$-dimensional vectors $\\mathbf u = [u_0,u_1,\\ldots,u_{d-1}]$ and $\\mathbf v = [v_0,v_1,\\ldots,v_{d-1}]$ is measured as\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "with $p(i)$ taking the value $2$ for even $i$, and $1$ for odd $i$. \n",
    "\n",
    "The indices in the returned list should be ordered by nondecreasing distance of the data points to `x`, i.e., `X[my_knn(x, X, 1)[0],:]` is a data point closest to `x`. If there are multiple points in `X` with the exact same distance to `x`, the returned indices should be increasing.\n",
    "\n",
    "**Example:** Assume that $k=4$ and the nearest neighbours to `x` are `X[7,:], X[2,:], X[9,:], X[0,:]` with distances $1.2,5.3,3.1,1.2$, respectively. Then the returned list should be `[0, 7, 9, 2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "267a76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_knn = None\n",
    "\n",
    "# TODO: Provide your solution code here that defines the function `my_knn`\n",
    "\n",
    "def my_knn(x, X, k):\n",
    "    import numpy as np\n",
    "\n",
    "    # Get the number of dimensions in the feature space\n",
    "    d = len(x)\n",
    "    \n",
    "    # Create the p values\n",
    "    p_value = [2 if i % 2 == 0 else 1 for i in range(d)]\n",
    "    \n",
    "    #Create an empty list to store the distances\n",
    "    distances = []\n",
    "\n",
    "    # Calculate the distance between x and each row in X\n",
    "    for row in X:\n",
    "        dist_u_v = 0\n",
    "        for i in range(d):\n",
    "            dist_u_v += (abs(x[i] - row[i]) ** p_value[i])\n",
    "        distances.append(dist_u_v)\n",
    "\n",
    "    # Get the indices of the k-nearest neighbours \n",
    "    indices = np.argsort(distances)\n",
    "    k_indices = indices[:k]\n",
    "    return k_indices.tolist()\n",
    "\n",
    "### Good to go \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b971ed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "import numpy as np\n",
    "def test_my_knn():\n",
    "    # Medium test case\n",
    "    x = np.array([3, 1])\n",
    "    X = np.array([\n",
    "        [1, 5], [2, 3], [3, 1], [4, 0], [3, 2], [0, 0], [2, 2], [4, 1], [5, 3], [3, 0]\n",
    "    ])\n",
    "    k = 4\n",
    "    result = my_knn(x, X, k)\n",
    "    expected = [2, 4 , 7 ,9]  # Example from the prompt\n",
    "    assert result == expected, f\"Expected {expected}, but got {list(result)}\"\n",
    "\n",
    "    # Small test cases\n",
    "    X_small = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "    x_small = np.array([4, 5])\n",
    "    k_small = 2\n",
    "    expected_small = [1, 2]  # Manually computed expected result\n",
    "    result_small = my_knn(x_small, X_small, k_small)\n",
    "    assert result_small == expected_small, f\"Failed small test case: {result_small}\"\n",
    "\n",
    "    # Large random test case\n",
    "    X_large = np.random.randint(0, 100, (1000, 5))  # 1000 points in 5D space\n",
    "    x_large = np.random.randint(0, 100, (5,))\n",
    "    k_large = 10\n",
    "    distances = [\n",
    "        (i, sum(abs(x_large[j] - X_large[i, j]) ** (2 if j % 2 == 0 else 1) for j in range(5)))\n",
    "        for i in range(1000)\n",
    "    ]\n",
    "    distances.sort(key=lambda x: (x[1], x[0]))  # Sort by distance, then by index\n",
    "    expected_large = [i[0] for i in distances[:k_large]]\n",
    "    result_large = my_knn(x_large, X_large, k_large)\n",
    "    assert result_large == expected_large, f\"Failed large test case: {result_large}\"\n",
    "\n",
    "    # Edge case: X containing duplicate points\n",
    "    X_duplicate = np.array([[1, 1], [1, 1], [2, 2], [3, 3]])\n",
    "    x_duplicate = np.array([1, 1])\n",
    "    k_duplicate = 3\n",
    "    expected_duplicate = [0, 1, 2]  # Since [1,1] is duplicated\n",
    "    result_duplicate = my_knn(x_duplicate, X_duplicate, k_duplicate)\n",
    "    assert result_duplicate == expected_duplicate, f\"Failed duplicate test case: {result_duplicate}\"\n",
    "\n",
    "    # Edge case: Single point dataset\n",
    "    X_single = np.array([[10, 10]])\n",
    "    x_single = np.array([10, 10])\n",
    "    k_single = 1\n",
    "    expected_single = [0]\n",
    "    result_single = my_knn(x_single, X_single, k_single)\n",
    "    assert result_single == expected_single, f\"Failed single point test case: {result_single}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "test_my_knn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dd3d5",
   "metadata": {},
   "source": [
    "## Problem 2b\n",
    "\n",
    "Using only plain Python with no modules except NumPy, write a function `my_knn_predict(x, X, k, y)` that takes the same inputs as the function in Problem 2a, as well as a Python list `y` with `N` elements (the labels of each data point). The function then returns a label (an element of `y`) that appeared most frequently among the $k$ nearest neighbors. If there are multiple labels with the same number of votes, a label with an associated feature closest to `x` is preferred. \n",
    "\n",
    "The function `my_knn_predict` may of course make use of `my_knn` by calling it.\n",
    "\n",
    "**Example:** Assume that $k=5$ and the labels of the nearest neighbours sorted by nondecreasing distance from `x` are `['c', 'b', 'a', 'a', 'b']`. In this case `b` should be returned, as `b` is one of the most frequent labels, and there is a data point labelled `b` which is potentially closer to `x` than the next data point labelled `a`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "15ce973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_knn_predict = None\n",
    "\n",
    "# TODO: Provide your solution code here that defines the function `my_knn_predict`\n",
    "\n",
    "\n",
    "def my_knn_predict(x, X, k, y):\n",
    "\n",
    "    # Get indices of k nearest neighbors\n",
    "    knn_indices = my_knn(x, X, k)\n",
    "\n",
    "    # Get the labels of the k nearest neighbors\n",
    "    knn_labels = [y[i] for i in knn_indices]\n",
    "    \n",
    "    # Count occurrences of each label\n",
    "    unique_labels, count = np.unique(knn_labels, return_counts=True)\n",
    "    \n",
    "    # Find the most frequent label(s)\n",
    "    max_count = np.max(count)\n",
    "    candidates = unique_labels[count == max_count]\n",
    "    \n",
    "    # Return if one most frequent neighbour\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]  \n",
    "    \n",
    "    # Return the label of the first tied frequent closest neighbor\n",
    "    for i in knn_indices:\n",
    "        if y[i] in candidates:\n",
    "            return y[i]  \n",
    "        \n",
    "### Looks  good. 99% certain it is  correct\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "df685c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def test_my_knn_predict():\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "\n",
    "    # Small test case\n",
    "    X_small = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "    y_small = ['A', 'B', 'B', 'A']\n",
    "    x_small = np.array([4, 5])\n",
    "    k_small = 2\n",
    "    expected_small = 'B'  # Nearest neighbors should be indices [1, 2], both labeled 'B'\n",
    "    result_small = my_knn_predict(x_small, X_small, k_small, y_small)\n",
    "    assert result_small == expected_small, f\"Failed small test case: {result_small}\"\n",
    "\n",
    "    # Large test case\n",
    "    X_large = np.random.randint(0, 100, (1000, 5))  # 1000 points in 5D space\n",
    "    y_large = np.random.choice(['A', 'B', 'C', 'D'], 1000)  # Random labels\n",
    "    x_large = np.random.randint(0, 100, (5,))\n",
    "    k_large = 10\n",
    "\n",
    "    # Get k-nearest neighbors using my_knn\n",
    "    knn_indices = my_knn(x_large, X_large, k_large)\n",
    "    knn_labels = [y_large[i] for i in knn_indices]\n",
    "\n",
    "    # Determine expected label\n",
    "    label_counts = Counter(knn_labels)\n",
    "    max_count = max(label_counts.values())\n",
    "    candidates = [label for label, count in label_counts.items() if count == max_count]\n",
    "\n",
    "    # Choose the closest label if there is a tie\n",
    "    expected_large = candidates[0]\n",
    "    if len(candidates) > 1:\n",
    "        for i in knn_indices:\n",
    "            if y_large[i] in candidates:\n",
    "                expected_large = y_large[i]\n",
    "                break\n",
    "\n",
    "    result_large = my_knn_predict(x_large, X_large, k_large, y_large)\n",
    "    assert result_large == expected_large, f\"Failed large test case: {result_large}\"\n",
    "\n",
    "    # Edge case - Tie-breaking\n",
    "    X_tie = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n",
    "    y_tie = ['A', 'B', 'A', 'B', 'C']\n",
    "    x_tie = np.array([3, 3])\n",
    "    k_tie = 4\n",
    "    expected_tie = 'A'  # A and B both appear twice, but index 2 (A) is closest\n",
    "    result_tie = my_knn_predict(x_tie, X_tie, k_tie, y_tie)\n",
    "    assert result_tie == expected_tie, f\"Failed tie-breaking test case: {result_tie}\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "    X = np.array([[1, 2], [2, 3], [1, 2], [1, 3], [2, 2], [3, 2]])\n",
    "    y = ['a', 'b', 'b', 'b', 'b', 'c']\n",
    "    x_test = np.array([1, 2])\n",
    "\n",
    "    # Assume k = 3\n",
    "    result = my_knn_predict(x_test, X, 3, y)\n",
    "    print(result)  # Expected: 'a' or 'b' depending on the distance\n",
    "\n",
    "# Run the tests\n",
    "test_my_knn_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de436669",
   "metadata": {},
   "source": [
    "## Problem 3a\n",
    "\n",
    "We will now work with some astronomical observation data used to classify celestial objects. Some missing values are given as $-9999$, and those are removed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "93697d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_ID</th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run_ID</th>\n",
       "      <th>rerun_ID</th>\n",
       "      <th>cam_col</th>\n",
       "      <th>field_ID</th>\n",
       "      <th>spec_obj_ID</th>\n",
       "      <th>class</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>MJD</th>\n",
       "      <th>fiber_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.237661e+18</td>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>3606</td>\n",
       "      <td>301</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>6.543777e+18</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.634794</td>\n",
       "      <td>5812</td>\n",
       "      <td>56354</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.237665e+18</td>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>4518</td>\n",
       "      <td>301</td>\n",
       "      <td>5</td>\n",
       "      <td>119</td>\n",
       "      <td>1.176014e+19</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.779136</td>\n",
       "      <td>10445</td>\n",
       "      <td>58158</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.237661e+18</td>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>3606</td>\n",
       "      <td>301</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>5.152200e+18</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.644195</td>\n",
       "      <td>4576</td>\n",
       "      <td>55592</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.237663e+18</td>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>4192</td>\n",
       "      <td>301</td>\n",
       "      <td>3</td>\n",
       "      <td>214</td>\n",
       "      <td>1.030107e+19</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.932346</td>\n",
       "      <td>9149</td>\n",
       "      <td>58039</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.237680e+18</td>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>8102</td>\n",
       "      <td>301</td>\n",
       "      <td>3</td>\n",
       "      <td>137</td>\n",
       "      <td>6.891865e+18</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.116123</td>\n",
       "      <td>6121</td>\n",
       "      <td>56187</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         obj_ID       alpha      delta         u         g         r  \\\n",
       "0  1.237661e+18  135.689107  32.494632  23.87882  22.27530  20.39501   \n",
       "1  1.237665e+18  144.826101  31.274185  24.77759  22.83188  22.58444   \n",
       "2  1.237661e+18  142.188790  35.582444  25.26307  22.66389  20.60976   \n",
       "3  1.237663e+18  338.741038  -0.402828  22.13682  23.77656  21.61162   \n",
       "4  1.237680e+18  345.282593  21.183866  19.43718  17.58028  16.49747   \n",
       "\n",
       "          i         z  run_ID  rerun_ID  cam_col  field_ID   spec_obj_ID  \\\n",
       "0  19.16573  18.79371    3606       301        2        79  6.543777e+18   \n",
       "1  21.16812  21.61427    4518       301        5       119  1.176014e+19   \n",
       "2  19.34857  18.94827    3606       301        2       120  5.152200e+18   \n",
       "3  20.50454  19.25010    4192       301        3       214  1.030107e+19   \n",
       "4  15.97711  15.54461    8102       301        3       137  6.891865e+18   \n",
       "\n",
       "    class  redshift  plate    MJD  fiber_ID  \n",
       "0  GALAXY  0.634794   5812  56354       171  \n",
       "1  GALAXY  0.779136  10445  58158       427  \n",
       "2  GALAXY  0.644195   4576  55592       299  \n",
       "3  GALAXY  0.932346   9149  58039       775  \n",
       "4  GALAXY  0.116123   6121  56187       842  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not change code in this cell\n",
    "astro = pd.read_csv(\"_datasets/star_classification.csv\")\n",
    "astro.replace(-9999, np.nan, inplace=True)\n",
    "astro.dropna(inplace=True)\n",
    "astro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9df5d",
   "metadata": {},
   "source": [
    "We only retain the columns named *u*, *g*, *r*, *i*, *z*, and *redshift*, and let `y` be the column *class*. We then split into training and testing data as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d891215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change code in this cell\n",
    "X = astro[[\"u\", \"g\", \"r\", \"i\", \"z\", \"redshift\"]]\n",
    "y = astro[\"class\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=3383 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77762268",
   "metadata": {},
   "source": [
    "For each value $k=2,3,\\ldots,6$, train a kNN classifier with $k$ neighbors, in a pipeline with z-score standardization, on the training set. Find the accuracy score of the trained model on the test. Produce a series `astro_scores` indexed by values of $k$ whose values are the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "407f0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "astro_scores = None\n",
    "\n",
    "# TODO: Provide your solution code here that defines `astro_scores`\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Train and evaluate kNN for k = 2 to 6\n",
    "scores = {}\n",
    "\n",
    "for k in range(2, 7):\n",
    "    # Create a pipeline with standardization and kNN classifier\n",
    "    model = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k))\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    scores[k] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Convert to pandas Series\n",
    "astro_scores = pd.Series(scores)\n",
    "\n",
    "### Looks good. Make sure it is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "12bba89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 Passed: astro_scores is a Pandas Series\n",
      "Test 2 Passed: astro_scores index contains expected values\n",
      "Test 3 Passed: Accuracy scores are valid\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_tests():\n",
    "    try:\n",
    "        # Ensure astro_scores is a Pandas Series\n",
    "        assert isinstance(astro_scores, pd.Series), \"astro_scores should be a Pandas Series\"\n",
    "        print(\"Test 1 Passed: astro_scores is a Pandas Series\")\n",
    "\n",
    "        # Ensure keys are within expected range\n",
    "        expected_keys = set(range(2, 7))\n",
    "        assert set(astro_scores.index) == expected_keys, \"astro_scores index should contain values from 2 to 6\"\n",
    "        print(\"Test 2 Passed: astro_scores index contains expected values\")\n",
    "\n",
    "        # Ensure accuracy scores are valid (between 0 and 1)\n",
    "        assert np.all((astro_scores >= 0) & (astro_scores <= 1)), \"All accuracy scores should be between 0 and 1\"\n",
    "        print(\"Test 3 Passed: Accuracy scores are valid\")\n",
    "\n",
    "        print(\"All tests passed successfully!\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test Failed: {e}\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ece137",
   "metadata": {},
   "source": [
    "## Problem 3b\n",
    "\n",
    "Build your own sklearn Pipeline called `astro_pipe` that classifies the data from Problem 3a. You can use any scaling functions or models as part of this pipeline. Tune the parameters to achieve highest possible classification accuracy on test sets made up of 20% of the overall data.\n",
    "\n",
    "There are two requirements:\n",
    "\n",
    "* training and prediction times should be below 20 seconds, respectively, to avoid timeouts\n",
    "\n",
    "* your final pipeline should be called `astro_pipe` and it should provide the usual `astro_pipe.fit()` and `astro_pipe.predict()` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "758b483e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9795\n"
     ]
    }
   ],
   "source": [
    "astro_pipe = None\n",
    "\n",
    "# TODO: Provide your solution code here that defines `astro_pipe`\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the pipeline with optimized parameters for faster training\n",
    "astro_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=840, max_depth=42, random_state=5460, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train the final pipeline\n",
    "astro_pipe.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = astro_pipe.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = (y_pred == y_test.values.ravel()).mean()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "### Looks good. Make sure it is correct \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d90090db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\twrob\\Desktop\\Year 3\\MAML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 5 features, but StandardScaler is expecting 6 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Expected behavior\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mtest_astro_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed for astro_pipe.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[109], line 18\u001b[0m, in \u001b[0;36mtest_astro_pipe\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Ensure the model is fitted and can make predictions\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mastro_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(preds) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction output length should match test input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError:\n",
      "File \u001b[1;32mc:\\Users\\twrob\\Desktop\\Year 3\\MAML\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:787\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 787\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\twrob\\Desktop\\Year 3\\MAML\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\twrob\\Desktop\\Year 3\\MAML\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\twrob\\Desktop\\Year 3\\MAML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\twrob\\Desktop\\Year 3\\MAML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 5 features, but StandardScaler is expecting 6 features as input."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "def test_astro_pipe():\n",
    "    # Create a small synthetic dataset\n",
    "    np.random.seed(42)\n",
    "    X_train = pd.DataFrame(np.random.rand(10, 5))  # 10 samples, 5 features\n",
    "    y_train = pd.Series(np.random.choice([\"A\", \"B\"], size=10))\n",
    "    X_test = pd.DataFrame(np.random.rand(5, 5))  # 5 test samples\n",
    "    \n",
    "    # Check that astro_pipe is a trained Pipeline\n",
    "    assert isinstance(astro_pipe, Pipeline), \"astro_pipe should return a Pipeline object.\"\n",
    "    \n",
    "    # Ensure the model is fitted and can make predictions\n",
    "    try:\n",
    "        preds = astro_pipe.predict(X_test)\n",
    "        assert len(preds) == 5, \"Prediction output length should match test input.\"\n",
    "    except NotFittedError:\n",
    "        assert False, \"astro_pipe is not fitted but should be.\"\n",
    "    \n",
    "    # Check invalid inputs\n",
    "    try:\n",
    "        astro_pipe.predict(pd.DataFrame())  # Empty dataset case\n",
    "        assert False, \"astro_pipe should raise an error on empty data.\"\n",
    "    except ValueError:\n",
    "        pass  # Expected behavior\n",
    "\n",
    "test_astro_pipe()\n",
    "print(\"All tests passed for astro_pipe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a227a",
   "metadata": {},
   "source": [
    "# End of test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30d8cb",
   "metadata": {},
   "source": [
    "You can use the below tests to get an indication if part of your work returns the right data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "281d2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKAY - student_mail appears to be valid\n",
      "OKAY - h_best_L1 should be a function\n",
      "OKAY - h_best_L1 returns a value\n",
      "OKAY - best_L1_err should be a function\n",
      "OKAY - best_L1_err returns a value\n",
      "OKAY - my_knn should be a function\n",
      "OKAY - my_knn returns a value\n",
      "OKAY - my_knn_predict returns a value\n",
      "OKAY - my_knn_predict should be a function\n",
      "OKAY - astro_scores should be a pandas series\n",
      "OKAY - astro_pipe should provide fit and predict methods\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    import re\n",
    "    assert re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', student_mail) and not 'firstname' in student_mail\n",
    "    print(\"OKAY - student_mail appears to be valid\")\n",
    "except:\n",
    "    print(\"WARN - student_mail could not be verified\")\n",
    "\n",
    "try: \n",
    "    assert callable(h_best_L1)\n",
    "    print(\"OKAY - h_best_L1 should be a function\")\n",
    "except:\n",
    "    print(\"FAIL - h_best_L1 should be a function\")\n",
    "\n",
    "import numpy as np\n",
    "X = np.array([[1,2,3],[2,-3,4],[1,2,3],[2,2,2]])\n",
    "Y = np.array([[1,2],[1,2],[3,4],[1,2]])\n",
    "x = np.array([1,2,3])\n",
    "\n",
    "try:\n",
    "    val = h_best_L1(x, X, Y)\n",
    "    assert val is not None\n",
    "    print(\"OKAY - h_best_L1 returns a value\")\n",
    "except:\n",
    "    print(\"FAIL - h_best_L1 does not return a value\")\n",
    "\n",
    "try: \n",
    "    assert callable(best_L1_err)\n",
    "    print(\"OKAY - best_L1_err should be a function\")\n",
    "except:\n",
    "    print(\"FAIL - best_L1_err should be a function\")\n",
    "\n",
    "try:\n",
    "    val = best_L1_err(X, Y)\n",
    "    assert val is not None\n",
    "    print(\"OKAY - best_L1_err returns a value\")\n",
    "except:\n",
    "    print(\"FAIL - best_L1_err does not return a value\")\n",
    "\n",
    "try: \n",
    "    assert callable(my_knn)\n",
    "    print(\"OKAY - my_knn should be a function\")\n",
    "except:\n",
    "    print(\"FAIL - my_knn should be a function\")\n",
    "\n",
    "try:\n",
    "    val = my_knn(x, X, k=2)\n",
    "    assert val is not None\n",
    "    print(\"OKAY - my_knn returns a value\")\n",
    "except:\n",
    "    print(\"FAIL - my_knn does not return a value\")\n",
    "\n",
    "try:\n",
    "    val = my_knn_predict(x, X, 2, x)\n",
    "    assert val is not None\n",
    "    print(\"OKAY - my_knn_predict returns a value\")\n",
    "except:\n",
    "    print(\"FAIL - my_knn_predict does not return a value\")\n",
    "\n",
    "try: \n",
    "    assert callable(my_knn_predict)\n",
    "    print(\"OKAY - my_knn_predict should be a function\")\n",
    "except:\n",
    "    print(\"FAIL - my_knn_predict should be a function\")\n",
    "\n",
    "try: \n",
    "    assert type(astro_scores) == pd.Series\n",
    "    print(\"OKAY - astro_scores should be a pandas series\")\n",
    "except:\n",
    "    print(\"FAIL - astro_scores should be a pandas series\")\n",
    "\n",
    "try: \n",
    "    assert callable(astro_pipe.fit) and callable(astro_pipe.predict)\n",
    "    print(\"OKAY - astro_pipe should provide fit and predict methods\")\n",
    "except:\n",
    "    print(\"FAIL - astro_pipe should provide fit and predict methods\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
